<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Looks by grantinverno</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Looks</h1>
        <p>Leandro Alberto-Dominguez: Senior Thesis in Eye Tracking </p>

        <p class="view"><a href="https://github.com/grantinverno/looks">View the Project on GitHub <small>grantinverno/looks</small></a></p>


        <ul>
          <li><a href="https://github.com/grantinverno/looks/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/grantinverno/looks/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/grantinverno/looks">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a id="abstract" class="anchor" href="#abstract" aria-hidden="true"><span class="octicon octicon-link"></span></a>Abstract</h1>

<p>In this research a novel method for minimizing the processing necessities, hardware necessities, and cost of interactive eye tracking based on computer vision is proposed. Created using Python, infrared LEDs, and a pair of small cameras, the method involves using Open Computer Visionâ€™s libraries to calculate the position of the pupil utilizing Hough Circle Transforms. Once this has been completed, the remaining camera is used as an environment camera, mapped from the pupil location data based on a bivariate polynomial that is formulated from calibration methods from multiple instances of use of the software. As the amount of users and the time used increases, as does the accuracy of the detection and mapping. </p>

<hr>

<hr>

<h1>
<a id="how-does-it-work" class="anchor" href="#how-does-it-work" aria-hidden="true"><span class="octicon octicon-link"></span></a>How Does it Work?</h1>

<p>The code utilizes a library known as Open Computer Vision which has the ability of analyzing individual video frames as well as, even more specifically, individual intensity values of each pixel. By manipulating and analyzing these intensity values, circles can be extracted from image frames. Once the circular shape of the iris is confirmed, the center can be generalized as the position of the pupil. Once the tracking is completed, the movement of the center point is related to the movement of the actual gaze. </p>

<h3>
<a id="hough-circle-transform" class="anchor" href="#hough-circle-transform" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hough Circle Transform</h3>

<p>The main process used for localizing the circle of the iris in the image plane, Hough Circle Transforms are a simple additive detection method based on probability of location. Essentially, each edge point that can potentially be part of a circle is mapped in another plane as a circle with its center at that coordinate. Once all the points are mapped, the point with the maximum number of intersections is most likely the center of a detected circle. </p>

<p><img src="http://lh3.googleusercontent.com/A32PiQfU7tHprbIazj5lnlh8ziT-n4jA-kC5YuMBa2Y=w457-h207-p-no" alt="Hough Circle Transform"></p>

<h3>
<a id="calibration" class="anchor" href="#calibration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Calibration</h3>

<p>The (x,y) coordinates of the detected iris center are individually mapped to a set of known x values and known y values. A polynomial fit is applied to the points for the x and the y set, creating two individual functions to describe the movement of the gaze x with respect the eye center x, and the gaze y with respect to the eye center y. </p>

<div class="highlight highlight-Python"><pre>q <span class="pl-k">=</span> cv2.waitKey(<span class="pl-c1">25</span>)
<span class="pl-k">if</span> q <span class="pl-k">==</span> <span class="pl-c1">ord</span>(<span class="pl-s"><span class="pl-pds">'</span>q<span class="pl-pds">'</span></span>):
    <span class="pl-k">if</span> <span class="pl-c1">len</span>(fx) <span class="pl-k">&lt;</span> <span class="pl-c1">3</span>:
        <span class="pl-k">if</span> circles <span class="pl-k">is</span>  <span class="pl-c1">None</span>:
               <span class="pl-k">print</span> <span class="pl-s"><span class="pl-pds">"</span>Did not find circles<span class="pl-pds">"</span></span>

        <span class="pl-k">else</span>:
               circles <span class="pl-k">=</span> np.uint16(np.around(circles))
               <span class="pl-k">for</span> (x,y,r) <span class="pl-k">in</span> circles[<span class="pl-c1">0</span>, :]:
                    fx.append(x)
                    <span class="pl-k">print</span> <span class="pl-s"><span class="pl-pds">"</span>X CALIBRATION POINT NUMBER<span class="pl-pds">"</span></span> <span class="pl-k">+</span> <span class="pl-c1">str</span>(<span class="pl-c1">len</span>(fx))
<span class="pl-s"><span class="pl-pds">'''</span></span>
<span class="pl-s">With each press of the 'q' key, the x coordinate </span>
<span class="pl-s">is added to the found x coordinate array, the</span>
<span class="pl-s">same is done with the w key and with the y coordinate array</span>
<span class="pl-s"></span>
<span class="pl-s"><span class="pl-pds">'''</span></span>

w <span class="pl-k">=</span> cv2.waitKey(<span class="pl-c1">25</span>)
<span class="pl-k">if</span> w <span class="pl-k">==</span> <span class="pl-c1">ord</span>(<span class="pl-s"><span class="pl-pds">'</span>w<span class="pl-pds">'</span></span>):
    <span class="pl-k">if</span> <span class="pl-c1">len</span>(fy) <span class="pl-k">&lt;</span> <span class="pl-c1">3</span>:
          <span class="pl-k">if</span> circles <span class="pl-k">is</span>  <span class="pl-c1">None</span>:
               <span class="pl-k">print</span> <span class="pl-s"><span class="pl-pds">"</span>Did not find circles<span class="pl-pds">"</span></span>

          <span class="pl-k">else</span>:
               circles <span class="pl-k">=</span> np.uint16(np.around(circles))
               <span class="pl-k">for</span> (x,y,r) <span class="pl-k">in</span> circles[<span class="pl-c1">0</span>, :]:
                    fy.append(y)
                    <span class="pl-k">print</span> <span class="pl-s"><span class="pl-pds">"</span>Y CALIBRATION POINT NUMBER<span class="pl-pds">"</span></span> <span class="pl-k">+</span> <span class="pl-c1">str</span>(<span class="pl-c1">len</span>(fy))
<span class="pl-s"><span class="pl-pds">'''</span></span>
<span class="pl-s">Once both of found X and found Y arrays are filled</span>
<span class="pl-s">the calibration sequence can commence</span>
<span class="pl-s">each pair is a step:</span>
<span class="pl-s"></span>
<span class="pl-s">1. Convert each found coordinate integer array into a NumPy array</span>
<span class="pl-s">2. Convert each known coordinate integer array into a NumPy array </span>
<span class="pl-s">3. Calculate the coefficients of the polynomial fit of the scatterplot</span>
<span class="pl-s">   of known coordinates against found coordinates.</span>
<span class="pl-s">4. Create a polynomial function with the calculated coefficients </span>
<span class="pl-s">5. Create a range for the graph of known y versus found y and the same</span>
<span class="pl-s">   for known x versus found x </span>
<span class="pl-s">6. Plot the two polynomials with all the points in the range</span>
<span class="pl-s">7. Show the plots.</span>
<span class="pl-s"><span class="pl-pds">'''</span></span>

<span class="pl-k">if</span> <span class="pl-c1">len</span>(fy) <span class="pl-k">==</span> <span class="pl-c1">3</span>:
   fx <span class="pl-k">=</span> np.array(fx)
   fy <span class="pl-k">=</span> np.array(fy)

   xcalib <span class="pl-k">=</span> np.array(xcalib)
   ycalib <span class="pl-k">=</span> np.array(ycalib)

   xcoefficients <span class="pl-k">=</span> np.polyfit(fx, xcalib, <span class="pl-c1">5</span>)
   ycoefficients <span class="pl-k">=</span> np.polyfit(fy, ycalib, <span class="pl-c1">5</span>)

   xpolynomial <span class="pl-k">=</span> np.poly1d(xcoefficients)
   ypolynomial <span class="pl-k">=</span> np.poly1d(ycoefficients)

   xs <span class="pl-k">=</span> np.arange(<span class="pl-c1">0</span>, <span class="pl-c1">640</span>)
   ys <span class="pl-k">=</span> np.arange(<span class="pl-c1">0</span>, <span class="pl-c1">480</span>)

   fxs <span class="pl-k">=</span> xpolynomial(xs)
   fys <span class="pl-k">=</span> ypolynomial(ys)


   yplot <span class="pl-k">=</span> figure()
   ax1 <span class="pl-k">=</span> yplot.add_subplot(<span class="pl-c1">111</span>)
   ax1.plot(ys, fys)
   ax1.scatter(ycalib, fy)
   xplot <span class="pl-k">=</span> figure()
   ax2 <span class="pl-k">=</span> xplot.add_subplot(<span class="pl-c1">111</span>)
   ax2.plot(xs, fxs)
   ax2.scatter(xcalib, fx)
   show()
</pre></div>

<h1>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Authors and Contributors</h1>

<p>Leandro Alberto - Dominguez
with special thanks to Pupil Labs and Carl Mills</p>

<h1>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span class="octicon octicon-link"></span></a>Support or Contact</h1>

<p>With questions or coding help (for me, I mean) email <a href="mailto:leandro.albertodominguez@gmail.com">leandro.albertodominguez@gmail.com</a></p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/grantinverno">grantinverno</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>