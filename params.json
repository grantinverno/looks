{"name":"Looks","tagline":"Leandro Alberto-Dominguez: Senior Thesis in Eye Tracking ","body":"# Abstract \r\nIn this research a novel method for minimizing the processing necessities, hardware necessities, and cost of interactive eye tracking based on computer vision is proposed. Created using Python, infrared LEDs, and a pair of small cameras, the method involves using Open Computer Visionâ€™s libraries to calculate the position of the pupil utilizing Hough Circle Transforms. Once this has been completed, the remaining camera is used as an environment camera, mapped from the pupil location data based on a bivariate polynomial that is formulated from calibration methods from multiple instances of use of the software. As the amount of users and the time used increases, as does the accuracy of the detection and mapping. \r\n\r\n\r\n***\r\n\r\n\r\n\r\n***\r\n# How Does it Work?\r\nThe code utilizes a library known as Open Computer Vision which has the ability of analyzing individual video frames as well as, even more specifically, individual intensity values of each pixel. By manipulating and analyzing these intensity values, circles can be extracted from image frames. Once the circular shape of the iris is confirmed, the center can be generalized as the position of the pupil. Once the tracking is completed, the movement of the center point is related to the movement of the actual gaze. \r\n\r\n### Hough Circle Transform\r\nThe main process used for localizing the circle of the iris in the image plane, Hough Circle Transforms are a simple additive detection method based on probability of location. Essentially, each edge point that can potentially be part of a circle is mapped in another plane as a circle with its center at that coordinate. Once all the points are mapped, the point with the maximum number of intersections is most likely the center of a detected circle. \r\n\r\n![Hough Circle Transform](http://lh3.googleusercontent.com/A32PiQfU7tHprbIazj5lnlh8ziT-n4jA-kC5YuMBa2Y=w457-h207-p-no)\r\n\r\n### Calibration\r\nThe (x,y) coordinates of the detected iris center are individually mapped to a set of known x values and known y values. A polynomial fit is applied to the points for the x and the y set, creating two individual functions to describe the movement of the gaze x with respect the eye center x, and the gaze y with respect to the eye center y. \r\n\r\n```Python\r\nq = cv2.waitKey(25)\r\nif q == ord('q'):\r\n    if len(fx) < 3:\r\n        if circles is  None:\r\n               print \"Did not find circles\"\r\n\r\n        else:\r\n               circles = np.uint16(np.around(circles))\r\n               for (x,y,r) in circles[0, :]:\r\n                    fx.append(x)\r\n                    print \"X CALIBRATION POINT NUMBER\" + str(len(fx))\r\n'''\r\nWith each press of the 'q' key, the x coordinate \r\nis added to the found x coordinate array, the\r\nsame is done with the w key and with the y coordinate array\r\n\r\n'''\r\n    \r\nw = cv2.waitKey(25)\r\nif w == ord('w'):\r\n    if len(fy) < 3:\r\n          if circles is  None:\r\n               print \"Did not find circles\"\r\n\r\n          else:\r\n               circles = np.uint16(np.around(circles))\r\n               for (x,y,r) in circles[0, :]:\r\n                    fy.append(y)\r\n                    print \"Y CALIBRATION POINT NUMBER\" + str(len(fy))\r\n'''\r\nOnce both of found X and found Y arrays are filled\r\nthe calibration sequence can commence\r\neach pair is a step:\r\n\r\n1. Convert each found coordinate integer array into a NumPy array\r\n2. Convert each known coordinate integer array into a NumPy array \r\n3. Calculate the coefficients of the polynomial fit of the scatterplot\r\n   of known coordinates against found coordinates.\r\n4. Create a polynomial function with the calculated coefficients \r\n5. Create a range for the graph of known y versus found y and the same\r\n   for known x versus found x \r\n6. Plot the two polynomials with all the points in the range\r\n7. Show the plots.\r\n'''\r\n    \r\nif len(fy) == 3:\r\n   fx = np.array(fx)\r\n   fy = np.array(fy)\r\n\r\n   xcalib = np.array(xcalib)\r\n   ycalib = np.array(ycalib)\r\n\r\n   xcoefficients = np.polyfit(fx, xcalib, 5)\r\n   ycoefficients = np.polyfit(fy, ycalib, 5)\r\n\r\n   xpolynomial = np.poly1d(xcoefficients)\r\n   ypolynomial = np.poly1d(ycoefficients)\r\n\r\n   xs = np.arange(0, 640)\r\n   ys = np.arange(0, 480)\r\n\r\n   fxs = xpolynomial(xs)\r\n   fys = ypolynomial(ys)\r\n\r\n\r\n   yplot = figure()\r\n   ax1 = yplot.add_subplot(111)\r\n   ax1.plot(ys, fys)\r\n   ax1.scatter(ycalib, fy)\r\n   xplot = figure()\r\n   ax2 = xplot.add_subplot(111)\r\n   ax2.plot(xs, fxs)\r\n   ax2.scatter(xcalib, fx)\r\n   show()\r\n\r\n```\r\n\r\n# Authors and Contributors \r\nLeandro Alberto - Dominguez\r\nwith special thanks to Pupil Labs and Carl Mills\r\n\r\n# Support or Contact\r\nWith questions or coding help (for me, I mean) email leandro.albertodominguez@gmail.com","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}